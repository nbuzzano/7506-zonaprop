{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from spark_sklearn import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import Dataset \n",
    "from lightgbm import train\n",
    "from lightgbm import cv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#levanto data sets\n",
    "df = pd.read_csv(\"../../machine-learning/cleanedData.csv\")\n",
    "y = df.precio\n",
    "\n",
    "X = pd.read_csv(\"../../machine-learning/xgboost-x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = X.join(y)\n",
    "aux = aux.dropna()\n",
    "aux = aux.sample(n=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aux = aux.precio\n",
    "#y_aux = np.log1p(y_aux)# applying log transformation here\n",
    "\n",
    "X_aux = aux.drop(['precio'], axis=1, inplace=False)\n",
    "X_aux.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "X_aux.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "y_test = pd.DataFrame(y_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aux, y_aux, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(y_test, ids, model):\n",
    "\t\tfinal_pred = y_test\n",
    "\n",
    "#\t\tids = self.df_test['id'].values\n",
    "\t\ttry:\n",
    "\t\t\tos.mkdir('predictions')\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\n",
    "\n",
    "\t\tsubmit = pd.DataFrame({'id':ids,'target':final_pred})\n",
    "\t\tsubmit.to_csv('../../machine-learning/predictions/submit-'+model+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "\t\tif not start_time:\n",
    "\t\t\tstart_time = datetime.now()\n",
    "\t\t\treturn start_time\n",
    "\t\telif start_time:\n",
    "\t\t\tthour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "\t\t\ttmin, tsec = divmod(temp_sec, 60)\n",
    "\t\t\tprint('Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    lgb_train = Dataset(X_train,y_train)#,free_raw_data=False)\n",
    "    lgb_eval = Dataset(X_test,y_test)#,free_raw_data=False)\n",
    "\n",
    "    return lgb_train,lgb_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_test_pred):\n",
    "    # mean_absolute_error\n",
    "\t\tmae = mean_absolute_error(y_test, y_test_pred)\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"mean_absolute_error (MAE): {0:.5f}\".format(mae)) \n",
    "\n",
    "\t\trms = sqrt(mae)\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"root mean_absolute_error (RMSE): {0:.5f}\".format(rms))     \n",
    "    \n",
    "    #calculating accuracy\n",
    "#\t\taccuracy_lgbm = accuracy_score(y_test_pred, y_test)\n",
    "#\t\tprint(\"LightGBM accuracy score: {0:.5f}\".format(accuracy_lgbm)) #ValueError: continuous is not supported\n",
    "#Is a classification or ranking metric, not a regression metric. So it doesn't accept continuous y.\n",
    "#https://github.com/scikit-learn/scikit-learn/issues/6592\n",
    "#\t\tprint(\"\")\n",
    "    \n",
    "    #calculating roc_auc_score for light gbm.\n",
    "#\t\tauc_lgbm = roc_auc_score(y_test,y_test_probs)\n",
    "#\t\tprint(\"LightGBM auc score: {0:.5f}\".format(auc_lgbm))\n",
    "#\t\tprint(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_nocv(lgb_train,lgb_eval):\n",
    "    \n",
    "\t\tparams = {'num_leaves':150,'max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "\t\tparams['metric'] = ['auc', 'mae', 'rmse']\n",
    "\n",
    "#\t\tparams = {\n",
    "##\t\t'num_leaves':15,\n",
    "#\t\t'boosting_type': 'gbdt',\n",
    "#\t\t'metric': 'mean_absolute_error',\n",
    "##\t\t'num_boost_round':4000,\n",
    "#\t\t'verbose': 0,\n",
    "#\t\t'learning_rate':0.22,\n",
    "##\t\t'max_depth':10\n",
    "#\t\t}\n",
    "\n",
    "\t\tprint('Start training...')\n",
    "\t\tstart_time = timer()\n",
    "\n",
    "\t\tbooster = train(\n",
    "\t\t\t\tparams=params,\n",
    "\t\t\t\ttrain_set=lgb_train,\n",
    "\t\t\t\tvalid_sets=[lgb_eval],\n",
    "\t\t\t\tverbose_eval=100,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\"\"\"booster = cv(\n",
    "\t\t\tparams=params,\n",
    "\t\t\ttrain_set=lgb_train,\n",
    "\t\t\t)\"\"\"\n",
    "\n",
    "\t\ttimer(start_time)\n",
    "\n",
    "\t\tprint(\"\")\n",
    "\t\tprint('Saving model into a pickle')\n",
    "\t\ttry:\n",
    "\t\t\tos.mkdir('pickles')\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\n",
    "\t\twith open('../../machine-learning/pickles/lightgbm.pkl','wb') as f:\n",
    "\t\t\tpickle.dump(booster, f)\n",
    "\n",
    "\t\tprint('Making prediction and saving into a csv')\n",
    "\t\ty_test_pred = booster.predict(X_test)\n",
    "\n",
    "#**************************************************************************************\n",
    "\n",
    "\t\tprint_metrics(y_test, y_test_pred)\n",
    "\n",
    "#**************************************************************************************\n",
    "\n",
    "\t\treturn y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train,lgb_eval = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[100]\tvalid_0's auc: 1\tvalid_0's l1: 526103\tvalid_0's rmse: 823471\n",
      "Time taken: 0 hours 0 minutes and 0.07 seconds.\n",
      "\n",
      "Saving model into a pickle\n",
      "Making prediction and saving into a csv\n",
      "\n",
      "mean_absolute_error (MAE): 526103.23560\n",
      "\n",
      "root mean_absolute_error (RMSE): 725.32974\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = train_model_nocv(lgb_train, lgb_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "PORQUE EL RMSE DE MI FUNCION print_metrics DA DISTINTO AL QUE IMPRIME lightgbm.train ???????\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = X_test.index\n",
    "save_prediction(y_test_pred, ids, 'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fijarme si documente otros !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedWriter name='../../machine-learning/pickles/lightgbm.pkl'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('../../machine-learning/pickles/lightgbm.pkl','wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
